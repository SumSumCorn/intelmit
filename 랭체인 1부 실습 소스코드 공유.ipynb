{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "본 문서는 아래 사이트에서 제공되는 설명 및 코드를 참고하였습니다. \n",
        "\n",
        "* https://python.langchain.com/"
      ],
      "metadata": {
        "id": "CjZxYI5cOBgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 사용자키 준비\n",
        "\n",
        "* https://openai.com/ 에서 OPENAI_API_KEY 키를 발급받아주세요.\n",
        "* https://serpapi.com/ 에서 SERPAPI_API_KEY 키를 발급받아주세요."
      ],
      "metadata": {
        "id": "hz_4SL4ghuFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
      ],
      "metadata": {
        "id": "6Tz11WhvICS7"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패키지 설치"
      ],
      "metadata": {
        "id": "PbukDq2dhxel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fWmF0RdHlb_",
        "outputId": "ada2d672-5cce-420f-9a8b-5b5480afc615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (0.0.138)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain  # 자연어 처리 및 기계 번역에 사용됩니다.\n",
        "!pip install tiktoken   # 텍스트 토큰화에 사용됩니다.\n",
        "!pip install openai     # OpenAI API와 상호작용하는 데 사용됩니다.\n",
        "!pip install google-search-results # 구글 검색 결과를 가져오는 데 사용됩니다.\n",
        "!pip install chromadb   # 벡터 데이터베이스를 이용하는 데 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 모델\n"
      ],
      "metadata": {
        "id": "z4r8jQYRLcoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. 대규모언어모델 (LLMs)"
      ],
      "metadata": {
        "id": "_gz7mk8DISvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 시작하기"
      ],
      "metadata": {
        "id": "Yw6z4f4yIiQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "text = \"인공지능팩토리 플랫폼은 인공지능을 위한 \bSNS이야. 이 플랫폼을 소개하기 위한 문구를 작성해\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHdAOoMAH794",
        "outputId": "c43c5473-e55d-48fc-90d2-cd701423704a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "보자면\n",
            "\n",
            "인공지능팩토리 플랫폼은 인공지능 기술의 연구 및 개발을 위해 딥러닝과 기계학습에 대한 연구 및 개발을 돕는 전문 SNS 플랫폼입니다. 사용자는 자신의 인공지능 프로젝트들을 공유하고 교류할 수 있고, 인공\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 반복해서 생성하기"
      ],
      "metadata": {
        "id": "rCirh-p5M5XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_result = llm.generate([\"개발자를 위한 챗GPT 활용 방법을 작성해\"]*3)\n",
        "\n",
        "len(llm_result.generations)\n",
        "\n",
        "for gen_texts in llm_result.generations:\n",
        "    print(gen_texts)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q14yJ3SN8fys",
        "outputId": "727b7d8a-9fae-4b6d-d4d8-68e3184fc68d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Generation(text='주세요.\\n\\n1. 좋은 챗GPT를 사용하기 위해서는 먼저 데이터를 잘 준비해야 합니다. 어떤 데이터를 사용할지 미리 정해놓고, 학습용 데이터에 맞춰 데이터 구성을 합니다.\\n\\n2. 챗GPT를 학습하기 위해서는 먼저 딥러닝 프레임워크에서 사', generation_info={'finish_reason': 'length', 'logprobs': None})]\n",
            "[Generation(text=' 주세요\\n\\n1. GPT를 사용하기 전에 당신이 시도하고자 하는 목적을 이해합니다. GPT는 사람과 같이 사고하는 기능을 갖추고 있기 때문에, 당신이 시도하고자 하는 목적을 정확하게 정의해 주어야 합니다.\\n\\n2. 당신이 원하는 목적을 정한 다음,', generation_info={'finish_reason': 'length', 'logprobs': None})]\n",
            "[Generation(text='보세요\\n\\n1. 챗GPT을 사용하기 전에 홈페이지에 제공하는 문서 등을 잘 읽어보고, 자신이 목표로 하는 개발 목적에 맞는 기능들을 이해합니다.\\n\\n2. 일단 챗GPT를 활용하기 위해 데이터 셋을 준비해야 합니다. 관련 키워드나 예시 문장,', generation_info={'finish_reason': 'length', 'logprobs': None})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_result.llm_output # 사용한 모델에 대한 정보 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT7xys0oM9H7",
        "outputId": "b530286c-7c76-4694-b8fd-25a65deeafdd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'token_usage': {'prompt_tokens': 135,\n",
              "  'completion_tokens': 767,\n",
              "  'total_tokens': 902},\n",
              " 'model_name': 'text-davinci-003'}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 토큰 개수 구하기"
      ],
      "metadata": {
        "id": "i7m8oxVhK2n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(\"Hello. I am Taeyoung Kim from AIFactory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AayE2KXYLH5d",
        "outputId": "ab9f0249-f1f4-4862-ad48-8c115a1d8faf"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(\"안녕하세요. 저는 인공지능팩토리의 김태영입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_byp7yIb7vVQ",
        "outputId": "4c0db236-d4bc-47cd-894d-859e770fde71"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. 채팅 모델"
      ],
      "metadata": {
        "id": "QSChuBEUMKCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "chat([HumanMessage(content=\"Translate this sentence from English to Korean. I love programming.\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_U2E-qvlMF5",
        "outputId": "f28c0257-708a-4f53-bf0a-9f29564e964a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='저는 프로그래밍을 좋아합니다.', additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to Korean.\"),\n",
        "    HumanMessage(content=\"Translate this sentence from English to Korean. I love programming.\")\n",
        "]\n",
        "\n",
        "chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "winqmhHhlOqg",
        "outputId": "60d4f913-15ff-4a3b-9c9e-3a252c161421"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='저는 프로그래밍을 좋아합니다.', additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to Korean.\"),\n",
        "        HumanMessage(content=\"Translate this sentence from English to Korean. I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to Korean.\"),\n",
        "        HumanMessage(content=\"Translate this sentence from English to Korean. I love artificial intelligence.\")\n",
        "    ],\n",
        "]\n",
        "\n",
        "result = chat.generate(batch_messages)\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY-r5JkzlQoA",
        "outputId": "8d301f81-95b7-45ee-843a-7ec3455248a7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(text='저는 프로그래밍을 좋아합니다.', generation_info=None, message=AIMessage(content='저는 프로그래밍을 좋아합니다.', additional_kwargs={}))], [ChatGeneration(text='나는 인공지능을 사랑합니다.', generation_info=None, message=AIMessage(content='나는 인공지능을 사랑합니다.', additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 73, 'completion_tokens': 26, 'total_tokens': 99}, 'model_name': 'gpt-3.5-turbo'})"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. 텍스트 임베딩 모델"
      ],
      "metadata": {
        "id": "umkgIV74N9Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "text = \"이 문서는 시험 문서입니다.\"\n",
        "\n",
        "query_result = embeddings.embed_query(text)\n",
        "doc_result = embeddings.embed_documents([text])\n",
        "\n",
        "print(query_result)\n",
        "print(doc_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adSdMvfQOEis",
        "outputId": "db3ccf55-be73-41ca-845d-8220a08b6d48"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.001326117837139328, -0.008679440781908096, -0.0007732924653825813, -0.022106383853839964, -0.01967958830267894, 0.011941691317853227, -0.018379992901444812, -0.004008190989876874, -0.029201115085801053, 0.002272634430882877, -0.0036501393857251317, -0.004127541835034966, -0.010801229135066514, -0.013645752644280858, 0.010071864825225745, 0.00930271627945059]\n",
            "[[-0.001326117837139328, -0.008679440781908096, -0.0007732924653825813, -0.022106383853839964, -0.01967958830267894, 0.011941691317853227, -0.018379992901444812, 0.029837649822235766, 0.03352425840134159, -0.01654994971342918, 0.007956706557174703, -0.004127541835034966, -0.010801229135066514, -0.013645752644280858, 0.010071864825225745, 0.00930271627945059]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 프롬프트"
      ],
      "metadata": {
        "id": "VrVo6hbWLjwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. 프롬프트 템플릿"
      ],
      "metadata": {
        "id": "QfLpgoYDh_WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"{product}에 적합한 제품명을 추천해\",\n",
        ")\n",
        "\n",
        "print(prompt.format(product=\"컬러 양말\"))\n",
        "print(prompt.format(product=\"스마트폰\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoaWB9_cIReH",
        "outputId": "057e6e3d-44d6-40c3-d876-598958b69b39"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "컬러 양말에 적합한 제품명을 추천해\n",
            "스마트폰에 적합한 제품명을 추천해\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "\n",
        "# First, create the list of few shot examples.\n",
        "examples = [\n",
        "    {\"word\": \"행복\", \"antonym\": \"슬픔\"},\n",
        "    {\"word\": \"길다\", \"antonym\": \"짧다\"},\n",
        "]\n",
        "\n",
        "# Next, we specify the template to format the examples we have provided.\n",
        "# We use the `PromptTemplate` class for this.\n",
        "example_formatter_template = \"\"\"\n",
        "word: {word}\n",
        "antonym: {antonym}\\n\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_formatter_template,\n",
        ")\n",
        "\n",
        "# Finally, we create the `FewShotPromptTemplate` object.\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    # These are the examples we want to insert into the prompt.\n",
        "    examples=examples,\n",
        "    # This is how we want to format the examples when we insert them into the prompt.\n",
        "    example_prompt=example_prompt,\n",
        "    # The prefix is some text that goes before the examples in the prompt.\n",
        "    # Usually, this consists of intructions.\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    # The suffix is some text that goes after the examples in the prompt.\n",
        "    # Usually, this is where the user input will go\n",
        "    suffix=\"Word: {input}\\nAntonym:\",\n",
        "    # The input variables are the variables that the overall prompt expects.\n",
        "    input_variables=[\"input\"],\n",
        "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n",
        "\n",
        "# We can now generate a prompt using the `format` method.\n",
        "print(few_shot_prompt.format(input=\"크다\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvqvRVQGOr6H",
        "outputId": "035ddc90-7c2e-4f28-e179-635299f4a1e7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "\n",
            "word: 행복\n",
            "antonym: 슬픔\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "word: 길다\n",
            "antonym: 짧다\n",
            "\n",
            "\n",
            "\n",
            "Word: 크다\n",
            "Antonym:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. 채팅 프롬프트 템플릿"
      ],
      "metadata": {
        "id": "we3WShqomT0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\").to_messages())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS3BLZbmmXLH",
        "outputId": "02fd5400-48c1-49c3-89e4-7946d3c3bdbf"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='저는 프로그래밍을 좋아합니다.', additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 체인"
      ],
      "metadata": {
        "id": "laNsJfSWfzr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. LLM과 프롬프트 템플릿을 이용한 체인"
      ],
      "metadata": {
        "id": "QYwDSlwVNQwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"service\"],\n",
        "    template=\"{service}에 AI을 활용할 수 있는 방안을 작성해\",\n",
        ")\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "chain.run(\"고객응답챗봇\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Xa-qMnqSIohJ",
        "outputId": "0491fca7-6154-40eb-cbdb-9bea86aa7180"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'보세요\\n\\n1. 빅데이터를 활용한 상담 자동화 : 데이터베이스를 관리하고 업데이트하여 고객 및 상품과 관련된 정보를 수집하여 빅데이터를 활용하여 고객과 상담 자동화를 수행할 수 있습니다.\\n\\n2. 시각-음성 인식 기술의 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. 채팅 모델과 프롬프트 템플릿을 이용한 체인\n"
      ],
      "metadata": {
        "id": "QnJUnOKMB9_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "\n",
        "chain.run(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GolOhCQ2NSvq",
        "outputId": "e5cd79ff-99fb-4002-9364-fa3aefd92ae2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'저는 프로그래밍을 좋아합니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 수학계산용 체인 - LLMMathChain"
      ],
      "metadata": {
        "id": "0iK96vudlsLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, LLMMathChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "llm_math = LLMMathChain(llm=llm, verbose=True)\n",
        "\n",
        "llm_math.run(\"What is 13 raised to the .3432 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "v5jncigjluil",
        "outputId": "566ec3a9-ceab-4151-fe59-b3810294aa0f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m\n",
            "```python\n",
            "import math\n",
            "print(math.pow(13, 0.3432))\n",
            "```\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\n",
            "\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: 2.4116004626599237\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 프롬프트 템플릿을 이용한 수학계산용 체인 - LLMMathChain"
      ],
      "metadata": {
        "id": "oWIzV1F3I-7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "_PROMPT_TEMPLATE = \"\"\"You are GPT-3, and you can't do math.\n",
        "\n",
        "You can do basic math, and your memorization abilities are impressive, but you can't do any complex calculations that a human could not do in their head. You also have an annoying tendency to just make up highly specific, but wrong, answers.\n",
        "\n",
        "So we hooked you up to a Python 3 kernel, and now you can execute code. If you execute code, you must print out the final answer using the print function. You MUST use the python package numpy to answer your question. You must import numpy as np.\n",
        "\n",
        "\n",
        "Question: ${{Question with hard calculation.}}\n",
        "```python\n",
        "${{Code that prints what you need to know}}\n",
        "print(${{code}})\n",
        "```\n",
        "```output\n",
        "${{Output of your code}}\n",
        "```\n",
        "Answer: ${{Answer}}\n",
        "\n",
        "Begin.\n",
        "\n",
        "Question: What is 37593 * 67?\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "print(np.multiply(37593, 67))\n",
        "```\n",
        "```output\n",
        "2518731\n",
        "```\n",
        "Answer: 2518731\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(input_variables=[\"question\"], template=_PROMPT_TEMPLATE)\n",
        "\n",
        "llm_math = LLMMathChain(llm=llm, prompt=PROMPT, verbose=True)\n",
        "\n",
        "llm_math.run(\"What is 13 raised to the .3432 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "MnJKENfEl0Hq",
        "outputId": "60e515ab-ae82-46be-8098-889a2a5b06d1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "print(np.power(13, .3432))\n",
            "```\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\n",
            "\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: 2.4116004626599237\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5. LLM기반 대화용 체인 - ConversationChain"
      ],
      "metadata": {
        "id": "ZlfZI5DDoNKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "Eeg76PpnoP9y",
        "outputId": "1fc64269-c630-458e-b545-f36317194e8a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi there! It's nice to meet you. My name is AI. What's your name?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "9E-aERDvoVPz",
        "outputId": "9d440531-276e-4bae-e397-7942dcc61ed6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6. 채팅 모델 기반 대화용 체인"
      ],
      "metadata": {
        "id": "xCzSM70som6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "G2Ocm0bVohaL",
        "outputId": "5fb5d7a3-1f0f-4964-f42f-95877eae8ec7"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'저는 프로그래밍을 좋아합니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7 스트리밍 (실시간 채팅)"
      ],
      "metadata": {
        "id": "DCF1n-TaNyp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.base import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "chat = ChatOpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)\n",
        "resp = chat([HumanMessage(content=\"Write me a song about sparkling water.\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cOJZ-KbN0ba",
        "outputId": "a1f68524-f682-42df-b027-4057b185c67b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verse 1:\n",
            "Bubbles rising to the top\n",
            "A refreshing drink that never stops\n",
            "Clear and crisp, it's oh so pure\n",
            "Sparkling water, I can't ignore\n",
            "\n",
            "Chorus:\n",
            "Sparkling water, oh how you shine\n",
            "A taste so clean, it's simply divine\n",
            "You quench my thirst, you make me feel alive\n",
            "Sparkling water, you're my favorite vibe\n",
            "\n",
            "Verse 2:\n",
            "No sugar, no calories, just H2O\n",
            "A drink that's good for me, don't you know\n",
            "With lemon or lime, you're even better\n",
            "Sparkling water, you're my forever\n",
            "\n",
            "Chorus:\n",
            "Sparkling water, oh how you shine\n",
            "A taste so clean, it's simply divine\n",
            "You quench my thirst, you make me feel alive\n",
            "Sparkling water, you're my favorite vibe\n",
            "\n",
            "Bridge:\n",
            "You're my go-to drink, day or night\n",
            "You make me feel so light\n",
            "I'll never give you up, you're my true love\n",
            "Sparkling water, you're sent from above\n",
            "\n",
            "Chorus:\n",
            "Sparkling water, oh how you shine\n",
            "A taste so clean, it's simply divine\n",
            "You quench my thirst, you make me feel alive\n",
            "Sparkling water, you're my favorite vibe\n",
            "\n",
            "Outro:\n",
            "Sparkling water, you're the one for me\n",
            "I'll never let you go, can't you see\n",
            "You're my drink of choice, forevermore\n",
            "Sparkling water, I adore."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 인덱스\n",
        "\n",
        "인덱스에 사용할 파일(\"state_of_the_union.txt\")을 다운로드 받습니다. "
      ],
      "metadata": {
        "id": "KItknQwPO6Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj_s1e6dKBO0",
        "outputId": "44ce0b0e-a668-4249-eabe-62cbac19de4f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-13 15:22:40--  https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39027 (38K) [text/plain]\n",
            "Saving to: ‘state_of_the_union.txt.1’\n",
            "\n",
            "state_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-04-13 15:22:41 (16.0 MB/s) - ‘state_of_the_union.txt.1’ saved [39027/39027]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. 파일기반 질의응답"
      ],
      "metadata": {
        "id": "DD5hZshJQDVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('state_of_the_union.txt')\n",
        "\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "print(index.query(query))\n",
        "\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "print(index.query_with_sources(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir3Q3OtaaLaT",
        "outputId": "33cc8b3b-4b7e-4fb2-93af-12bff8aed123"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\n",
            "{'question': 'What did the president say about Ketanji Brown Jackson', 'answer': \" The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, one of the nation's top legal minds, to continue Justice Breyer's legacy of excellence, and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\n\", 'sources': 'state_of_the_union.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. 벡터DB기반 질의응답"
      ],
      "metadata": {
        "id": "VNHIwdgBaKPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### stuff 방식 - 기본"
      ],
      "metadata": {
        "id": "ZizPA0W_qZ3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.indexes.vectorstore import VectorstoreIndexCreator"
      ],
      "metadata": {
        "id": "hVqblL__PG1H"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"state_of_the_union.txt\") as f:\n",
        "    state_of_the_union = f.read()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()\n",
        "\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "iFiLS2YyQQwu",
        "outputId": "caec7b52-086a-4e73-f3af-85d45f01ce46"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The president said that Justice Breyer is an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court, and thanked him for his service.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### stuff 방식 - 사용자 정의 프롬프트 템플릿"
      ],
      "metadata": {
        "id": "VDEMMSuTQgGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer in Korean:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekYUlEvdQTEI",
        "outputId": "348c8692-82ce-4a02-cdb8-c3049d6b017d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output_text': ' 대통령은 스티븐 브라이어 재판관에 대해 말했다, 그는 군인, 헌법학자 및 미국 최고법원의 퇴임 재판관이며 이 나라를 섬기기 위해 삶을 헌신한 사람이라고 기념하고 싶다고 말했다.'}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### map-rerank 방식 - 기본"
      ],
      "metadata": {
        "id": "yCW0Vkw_bm4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", return_intermediate_steps=True)\n",
        "\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "results = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
        "\n",
        "results[\"output_text\"]\n",
        "\n",
        "results[\"intermediate_steps\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nYr20OCbmdp",
        "outputId": "45512968-8cbc-4d44-cfae-b50052931187"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': ' The President thanked Justice Breyer for his service and honored him for dedicating his life to serve the country.',\n",
              "  'score': '100'},\n",
              " {'answer': ' This document does not answer the question', 'score': '0'},\n",
              " {'answer': ' This document does not answer the question', 'score': '0'},\n",
              " {'answer': ' This document does not answer the question', 'score': '0'}]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### map-rerank - 사용자 정의 프롬프트 템플릿"
      ],
      "metadata": {
        "id": "bRVD-3xdqj2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import RegexParser\n",
        "\n",
        "output_parser = RegexParser(\n",
        "    regex=r\"(.*?)\\nScore: (.*)\",\n",
        "    output_keys=[\"answer\", \"score\"],\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
        "\n",
        "Question: [question here]\n",
        "Helpful Answer In Italian: [answer here]\n",
        "Score: [score between 0 and 100]\n",
        "\n",
        "Begin!\n",
        "\n",
        "Context:\n",
        "---------\n",
        "{context}\n",
        "---------\n",
        "Question: {question}\n",
        "Helpful Answer In Korean:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    output_parser=output_parser,\n",
        ")\n",
        "\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", return_intermediate_steps=True, prompt=PROMPT)\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ud4HV8cbwFn",
        "outputId": "09b32795-f4e9-4c3f-879a-8cccb177adc2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intermediate_steps': [{'answer': ' 대통령은 스티븐 브라이어 재판관에 대해 무엇을 말했습니까?',\n",
              "   'score': '100'},\n",
              "  {'answer': ' 대통령은 브라이어 재판관에 대해 무엇을 말했습니까?', 'score': '0'},\n",
              "  {'answer': ' 대통령은 브라이어 재판관에 대해 무엇을 말했습니까?', 'score': '0'},\n",
              "  {'answer': ' 대통령은 저스티스 브라이어에 대해 무엇을 말했습니까?', 'score': '0'}],\n",
              " 'output_text': ' 대통령은 스티븐 브라이어 재판관에 대해 무엇을 말했습니까?'}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. RetrievalQA 체인을 이용한 검색 기반 질의응답"
      ],
      "metadata": {
        "id": "xTm-l8gscJ2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader(\"state_of_the_union.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
        "\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "UJgLpvdYcLIf",
        "outputId": "9e732f1f-4b24-4c1d-b07c-7ecad104ce60"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a consensus builder who has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. He said she is a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. RetrievalQA 체인과 프롬프트 템플릿을 이용한 검색 기반 질의응답"
      ],
      "metadata": {
        "id": "yz4MT_1GLdOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer in Korean:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "sBWyWqp-cb30",
        "outputId": "71baf571-e843-43d9-a253-1c9a3fc5624c"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" 대통령은 케탄지 브론 잭슨에 대해 무엇을 말했나요? \\nAnswer in English: The President said that Judge Ketanji Brown Jackson is one of the nation's top legal minds, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5. 요약체인과 문서분석체인을 이용한 요약"
      ],
      "metadata": {
        "id": "lX03HODagDas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"state_of_the_union.txt\") as f:\n",
        "    state_of_the_union = f.read()\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "from langchain.chains import AnalyzeDocumentChain\n",
        "\n",
        "summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)\n",
        "\n",
        "summarize_document_chain.run(state_of_the_union)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "irRzankagHSU",
        "outputId": "0752494c-a343-43c1-c642-c1f4fa8242f9"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" In this speech, President Biden addresses the American people and the world, discussing the recent aggression of Russia's Vladimir Putin in Ukraine and the US response. He outlines economic sanctions and other measures taken to hold Putin accountable, and announces the US Department of Justice's task force to go after the crimes of Russian oligarchs. He also outlines plans to invest in America, modernize infrastructure, and promote environmental justice, and proposes a plan to fight inflation. He calls for the continued combat of COVID-19, and proposes a Unity Agenda for the Nation. He also provides assistance to lower-income veterans and leads the Cancer Moonshot. He concludes by emphasizing the strength of the nation and the need to come together to overcome the challenges of our time.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6. 질의응답체인과 문서분석체인을 이용한 질의응답\n"
      ],
      "metadata": {
        "id": "CCkLpLb9gVwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\n",
        "\n",
        "qa_document_chain.run(input_document=state_of_the_union, question=\"what did the president say about justice breyer?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QiOG65URgZet",
        "outputId": "4c37b7d2-5be7-4006-8038-bc13453609d8"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The president thanked Justice Breyer for his service.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 메모리"
      ],
      "metadata": {
        "id": "XZ8T7wCQia6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. 메모리를 이용한 챗봇"
      ],
      "metadata": {
        "id": "EZVj_QGld0UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "\n",
        "template = \"\"\"You are a chatbot having a conversation with a human.\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "Chatbot:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\"], \n",
        "    template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=OpenAI(), \n",
        "    prompt=prompt, \n",
        "    verbose=True, \n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "llm_chain.predict(human_input=\"Hi there my friend\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "cvxpQAtxdzpQ",
        "outputId": "13bbad4e-e4c1-4938-f0d4-6eaef6bee64a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
            "\n",
            "\n",
            "Human: Hi there my friend\n",
            "Chatbot:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hi there, how are you doing today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(human_input=\"Not to bad - how are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "QhNS7nwYd8dq",
        "outputId": "5a68351a-6942-487d-d242-d3b92f0e92ec"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
            "\n",
            "Human: Hi there my friend\n",
            "AI:  Hi there, how are you doing today?\n",
            "Human: Not to bad - how are you?\n",
            "Chatbot:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm doing great, thanks for asking!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 에이전트"
      ],
      "metadata": {
        "id": "-0sulZPHgsnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1. 구글 검색 도구"
      ],
      "metadata": {
        "id": "PwQTtqdPhPLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utilities import SerpAPIWrapper\n",
        "\n",
        "search = SerpAPIWrapper()\n",
        "search.run(\"Obama's first name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IbkLYIk0hRIm",
        "outputId": "7ee5f86c-733d-4fa1-ba63-3cbfd9af39d1"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Barack Hussein Obama II'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"engine\": \"bing\",\n",
        "    \"gl\": \"us\",\n",
        "    \"hl\": \"en\",\n",
        "}\n",
        "search = SerpAPIWrapper(params=params)\n",
        "\n",
        "search.run(\"Obama's first name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "7Xt8W69ShXeU",
        "outputId": "4bdba10e-5916-40dc-c38a-bc8c0a92bec5"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Barack Hussein Obama II is an American former politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African-Ame…New content will be added above the current area of focus upon selectionBarack Hussein Obama II is an American former politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African-American president of the United States. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004, and worked as a civil rights lawyer before holding public office.Wikipediabarackobama.com'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2. 구글 검색 도구와 수학 도구를 이용한 에이전트"
      ],
      "metadata": {
        "id": "edzDDj0vniMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "UKK-_-FMnhvf",
        "outputId": "b0a58ea5-3c2b-4114-b42b-022cd45f6be8"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
            "Action: Search\n",
            "Action Input: \"High temperature in SF yesterday\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHigh: 59ºf @3:40 PM Low: 48.02ºf @5:56 AM Approx. Precipitation / Rain Total: in. 1hr.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to use the calculator to raise the temperature to the .023 power.\n",
            "Action: Calculator\n",
            "Action Input: 59^.023\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.0983217810400328\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0983217810400328.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0983217810400328.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3. 채팅 모델을 이용한 에이전트"
      ],
      "metadata": {
        "id": "V2i0udciKiJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
      ],
      "metadata": {
        "id": "eji16h06KgHP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "e54cac47-1c75-4b53-fe38-2165b954c7ca"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Search\",\n",
            "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mOlivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mNow I need to use a calculator to raise Harry Styles' age to the 0.23 power.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"pow(27, 0.23)\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.1340945944237553\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "Final Answer: 2.1340945944237553\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1340945944237553'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    }
  ]
}
